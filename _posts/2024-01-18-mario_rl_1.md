---
layout: post
title:  "[Mario Bros RL-1] The Gym setup!"
date:   2024-01-18 21:00:00 +0100
categories: Reinforcement Learning, Machine Learning, PPO, DDQN, Gym, Gymnasium
image: /assets/img/mario_rl_mini.png
---

<!-- https://giphy.com/gifs/x77AsI0PPARBrrO6vK -->
{% include giphy.html id='x77AsI0PPARBrrO6vK' %}
*Best attempt so far using a DDQN (after 29155 games)*

## Mario Bros as a Reinforcement Learning Challenge

First, let's examine our main tool, the OpenAI'the Gym platform.
It runs simulations and provides feedback from them.
We'll learn how to get feedback from a Gym environment. 
This includes interacting with the emulator to receive rewards and understanding how these rewards are calculated. 

In the future, we might choose not be able to use the Gym platform (for example, for a different challenge). 
Then, we'll have to handle these aspects ourselves.

### The Gym plateform and Nes-py emulator

Standard plateforms to learn reinforcement learning include [OpenAI Gym API](https://www.gymlibrary.dev/index.html) and its successor [Gymnasium](https://gymnasium.farama.org/).
They offer a [Super Mario Bros. environment](https://pypi.org/project/gym-super-mario-bros/) based on the [Nes-py emulator](https://github.com/Kautenja/nes-py)

This Mario Bros environment is an ideal testing ground due to its balance between simplicity and complexity.
Its traps and enemies provide clear, yet challenging objectives for algorithms.
The main complexities include:

- **Game Physics**: The game's internal physics adds complexity to movement and interaction.
- **Complex Reward**: Rewards depends on factors like level completion, remaining time, etc.
- **In-Game Challenges**: The presence of traps, such as holes, enemies, and pipes, creates a challenging environment for algorithms.

Plus, using raw images and outputs can be inefficient and overly complex.
In many frames, numerous pixels don't carry useful information. 
To make the learning process more efficient, the Gym plateform provides simplifications to interact with the game:

- **Emulator**: the environment provide us an emulator (Nes-py) to run our simulation.
It also wraps the emulator API in a simple way to retrieve frames and send controller inputs.
- **Simplified output**: We can choose each available output actions for the model. 
- **Rewards system**: The environment will provide rewards aligned with the game's objective. We'll explain that in the next section.
- **Environment abstration**: Remove loading screens and cut-scenes to focus solely on gameplay.
- **Data reduction**: Use a lower resolution version of the game to reduce input complexity. We'll use the `SuperMarioBros-v3` simplified version.

![Simplified environment](https://pypi-camo.freetls.fastly.net/51975e7cc634efb02ed92acfb56368733b25f4d9/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f323138343436392f34303934383831372d33636436363030612d363833302d313165382d386162622d3963656536613331643337372e706e67)

We will create a neural network that processes raw game images as input and translate them into gamepad events as output:
- **Input**: Initially the input requires a large number of neurons, one for each piece of picel information in a game frame -
Typically (240 x 256 x 3) or 184.320 input neurons.
By downsampling the frame resolution and convert it to grayscale, we'll simplifying this to (30 x 32 x 1), or 960 neurons.
- **Output**: The number of output neurons will correspond to the possible actions. 
Each neuron will represents a different action, like 'push the move right button', 'push the jump button', etc.
We'll use a simplified output mode called `RIGHT_ONLY`, which has 5 possibilities: 
```python
[['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B']]
```
`Noop` is for no input. 
These simplifications significantly reduces the complexity of the input and output spaces.

In the Gym framework, the concept is to use a base environment that will provide the simulation.
Then it is to add some `wrappers` on top of it.
Each wrapper will modify the environment and add some features.

```python
# Create the base environment
env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3')
# Setup the joypad space to the simplified output mode
env = JoypadSpace(env, RIGHT_ONLY)
# The SkipFrame wrapper will skip some frames and cumulate the reward during these frames.
# So the agent take a decision every four frames
env = SkipFrame(env, skip=4)
# The ResizeObservation permits to resize the frames given to the agent, keeping the ration and setting the width to 30px
env = ResizeObservation(env, shape=30)
# Convert the frames to a gray scale
env = GrayScaleObservation(env, keep_dim=False)
# Cumulate the last four frames to be given as input to the agent
env = FrameStack(env, num_stack=4, lz4_compress=True)
```

Here is how to interact with the game simulation using the Gym plateform:

```python
# initial state
done = False
state = env.reset()
# main loop
while not done: 
    
    # Here we get a random action 
    # In practice the agent would select an action based on the current state
    # For instance: action = model.predict_action(state)
    action = env.action_space.sample()

    # The environment execute the agent action and returns the new state, the reward and whether the game is over
    new_state, reward, done, info = env.step(action)

    # Will render the new_state
    env.render()

    # Update the current state to the new state
    state = new_state
```

In the described setup, game frames are used as states in the environment:
- `state`: is the previous frame.
- `new_frame` is the next one after the agent action.

Additionally, `Info`  a dictionary that holds various current game information, useful for creating custom reward functions or for tracking the progress of the agent:
- `flag_get`: indicates if the player has completed the level.
- The count of collected coins.
- The number of remaining lifes.
- The in-game score so far.
- The Mario's status: `small`, `tall`, `fireball`, etc., indicating his current power-up state.
- the remaining time on the clock.
- `x_pos` and `y_pos`: these represent Mario's position on the stage, providing spatial context within the game

Understanding this information can be crucial for developing custom reward strategy.
For now, it's important to first comprehend how the current reward is computed within the environment.

### How the reward is computed?

The reward is a critical point in the learning process. Let's review how it is computed.
From the [Gym Super Mario Bros page](https://pypi.org/project/gym-super-mario-bros/):

    The reward function assumes the objective of the game is to move as far right as possible (increase the agent's x value), as fast as possible, without dying.

The reward system in this Mario Bros game environment is structured as a sum of three components:
One positive: the velocity, and two negatives: the clock and the death:

1. The `velocity`, noted $v$, is the instantaneous velocity for the given step. It is computed by taking the difference in agent x values between states: $v = x1 - x0$, where:
  - $ x0 $ is the x position before the step
  - $x1$ is the x position after the step
  - This implies that, when moving right: $v > 0$, when moving left: $v < 0$ and not moving: $v = 0$.

2. The `clock`, noted $c$, is a penalty that prevents the agent from standing still.
We want that when no clock tick: $c = 0$ and then there is a clock tick: $c < 0$.
This is the difference in the game clock between frames: $c = c0 - c1$, where:
  - $c0$ is the clock reading before the step
  - $c1$ is the clock reading after the step

3. The `death` part, denoted $d$, penalizes for dying. This is to encouraging the agent to avoid death. We simply want that whenn the agent is alive: $d = 0$ and when it lost a live: $d = -15$. 

The total reward, $r$, is simply the sum at each frame of the three terms: $r = v + c + d$.
The reward is also clipped into the range (-15, 15). 
This clipping is typically employed to maintain stability in the learning process.
By constraining the reward to a fixed range, it prevents the learning algorithm from being influenced by extreme reward values.

## Next steps

In the following articles we'll talk about algorithms I'd like to play with: Double Deep Q Networks (DDQN), Proximal Policy Optimization (PPO, which is interesting because it's the algorithm behind RLHF used to train the GPT models) and probably NeuroEvolution of Augmenting Topologies (NEAT, which is not an reinforment learning algorithm, it doesn't use the error retropropagation, but seems cool and very efficient).

In the [next article]({{ site.baseurl }}{% post_url 2024-01-25-mario_rl_2 %}) we will talk about DDQN and how it works.

Thanks and see you next time!