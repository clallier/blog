---
layout: post
title:  "Learning to play Mario Bros with reinforcement!"
date:   2023-12-01 21:00:00 +0100
categories: Reinforcement Learning, Machine Learning, PPO, DDQN
image: /assets/img/mario_rl_mini.png
---

Let's have a computer to play `Mario Bros` using reinforcement learning.
In this experiment, we'll explore neural networks and test if a computer can master the game.
Ready to level up?

## Introduction

This is a project I wanted to start since a long time.
Neural networks have always fascinated me.
My PhD research was about applying neural network on graphs, but this field is large.
For example, I've never explored the well known reinforcement learning (RL).

In machine learning, there are two main ways to teach an algorithm:
1. **Supervised learning**: this is the most intuitive. 
It's like teaching a kid to recognise letters. 
For each letter you may encounter, you tell them `this is an 'A'`, `this is a 'B'`, etc.

2. **unsupervised learning**: algorithms disover patterns on their own. 
Think of it like children playing with shape sorter toys. 
They learn to match shapes with the right holes, focusing on important features like the shape's contours, while ignoring irrelevant ones like color or material.

Reinforcement learning offer a third approach. 
In this method, an algorithm learns to perform a task by itself and rewards from the environment.
Video games are an ideal fit for this method: each in-game action will result in a modification of the score, providing a reward, that will influence the learning algorithm to either explore or exploit the game's environment.

**Neural network**: are special case of learning agorithms, based on imitation of biological neural network, structured in layers.
 - The first layer is the `input`: it's where the input come,
 - The last layer is the `ouput`: that's the output of the network.
 If we want a network to predict a letter, it will have 26 output neurons. 
 Each neuron represent a different letter.
 - All the intermediate layers are the `compute` of the network. 
 When an input comes from the input layer, it flows through the intermediate layers and activate some neurons on the output layers.
 - We can see each neuron as connected to some of the neurons of the following layer.
 Basically when a neuron is `actived` (receive a value) it will activate its followers with a certain force.   
 Where the learning is done using the error backpropagation. 

My goal with this project is to deepen my understanding of reinforcement learning and its main algorithms, and to share about what I learnt.

It's a neat challenge to see if such an algorithm can eventually solve the game.

What we'll attenpt to make:

<!-- https://giphy.com/gifs/x77AsI0PPARBrrO6vK -->
{% include giphy.html id='x77AsI0PPARBrrO6vK' %}
*Best attempt so far using a DDQN (after 29155 games)*


## Mario Bros as an RL Challenge

A standard environment to learn reinforcement learning is [OpenAI Gym API](https://www.gymlibrary.dev/index.html) and its successor [Gymnasium](https://gymnasium.farama.org/).
 They provide a [Mario Bros environment](https://pypi.org/project/gym-super-mario-bros/)

This environment seems to be a good testing ground because it is balanced between simplicity and complexity: its traps and enemies provide clear yet challenging objectives for an algorithm.



neural network that processes raw game images

https://github.com/Sourish07/Super-Mario-Bros-RL


We use a simplifed environment: 
- Simplified output state: 7 instead of 36 $6^2$ ?  
- rewards: the env gives reward if you go right
- No loading screen or cut-scene
- Down sampled version of the game

Key Point: Why Mario Bros is a challenging game for RL algorithms.
- Extracting information from raw images given by the emulator
- Input space is very large (240 x 256 x 3)
- Output space is 5 with `RIGHT_ONLY`
- Internal physics
- Reward is complicated: end of the level, timer, etc 
- Traps: holes, enemies and pipes

```python
# initial state
done, state = False, env.reset()
# main loop
while not done: 
    # the agent select an action given the state
    action = model.predict_action(state)
    # the environment execute the agent action and returns the new state, the reward and if the game is over
    new_state, reward, done = env.step(action)

    # store everything in the replay buffer
    model.replay_buffer.store(state, action, reward, new_state, done)
    model.learn()

    # setting the new_state as the actual state
    state = new_state
```
## How the reward are computed?

## Main problems to solve

- The environment is complicated: 
  - often, the player is stcuk by the pipes
  - there are death trap (hole in the ground)
  - an enemies

- The example shown at the top of the page is a good episode, but it doesn't mean the model has converged to a state where it plays only good episodes. 

## Next steps

In the next article we will talk about DDQN and how it works.
In the following articles we will talk about other algorithms I'd like to try: PPO and probably Neat(? neuroevolution)
PPO is interesting because it's the algorithm behind RLHF used to train the GPT models 

See you next time