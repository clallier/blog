---
layout: post
title:  "Learning to play Mario Bros with reinforcement!"
date:   2024-01-13 21:00:00 +0100
categories: Reinforcement Learning, Machine Learning, PPO, DDQN
image: /assets/img/mario_rl_mini.png
---

Let's have a computer to play `Mario Bros` using reinforcement learning.
In this experiment, we'll explore neural networks and test if a computer can master the game.
Ready to level up?

<!-- https://giphy.com/gifs/x77AsI0PPARBrrO6vK -->
{% include giphy.html id='x77AsI0PPARBrrO6vK' %}
*Best attempt so far using a DDQN (after 29155 games)*


## Introduction

Learning to play a game, such as Mario Bros, using reinforcement leanring is a project I wanted to start for a long time.
Neural networks have always fascinated me.
My PhD research focused on applying neural network to graphs, but the field of neural network is remarkably vast.
For example, I've never explored the world of reinforcement learning (RL).

In machine learning, there are three main ways to teach an algorithm:
1. **Supervised learning**: this is the most intuitive method. 
It's similar to teaching a child to recognise letters. 
For each letter they might encounter, you would say `this is an 'A'`, `this is a 'B'`, and so on.

2. **Unsupervised learning**: here, algorithms discover patterns on their own. 
Imagine children playing with shape-sorter toys. 
They figure out how to match shapes with the correct holes by focusing on key features like the shape, while ignoring irrelevant ones like color or material.

3. **Reinforcement learning**: in this approach, an algorithm learns to perform tasks on its own by receiving rewards from the environment.
Video games are an ideal fit for this method: each in-game action will result in a modification of the score, providing a reward, which guide the learning algorithm to explore and exploit the game's environment.

We talked about machine learning and neural networks but what is the difference ?
 
On one hand, `machine learning` is a broad concept: it includes any algorithms that learn to resolve a given task.
In this context, learning is the action of adjusting the internal values of the algorithm.
**Neural network** are a specific type of learning agorithms.
They are a simplified version of biological neural networks, and are usually structured in layers.
 - The first layer is the `input`: it's where the data enters. 
 For example, in a network designed to recognize letters from images.
 their would be one neuron for each pixel value. 
 - The last layer is the `ouput` of the network. 
 In our letter recognition example, it will have 26 output neurons. 
 Each representing a different letter.
 - All the intermediate layers do the network's calculations. 
 - Each neuron connects to some neurons in the next layer.
 When data enters the input layer, it travels through each layers and activate some neurons in the output layers.
 - A neuron receives an `activation level` (a value) and propagate its activation to the next neurons.   
 - Learning occus through a process called `error backpropagation`, which alter how a neuron propagates its activation.
 This process turns on or off each neuron's activation on the following neurons.

My goal with this project is to deepen my understanding of reinforcement learning and its main algorithms, and to share about what I learnt.

It's a neat challenge to see if such an algorithm can eventually solve the game.

## Mario Bros as an RL Challenge

### The gym plateform and Nes-py emulator
Some standard plateforms to learn reinforcement learning are [OpenAI Gym API](https://www.gymlibrary.dev/index.html) and its successor [Gymnasium](https://gymnasium.farama.org/).
They provide a [Mario Bros environment](https://pypi.org/project/gym-super-mario-bros/) itself based on the [Nes-py emulator](https://github.com/Kautenja/nes-py)

This environment is an ideal testing ground due to its balance between simplicity and complexity: 
its traps and enemies provide clear yet challenging objectives for an algorithm.
The main complexities reside in:

- **Game Physics**: The internal physics of the game adds complexity to movement and interaction.
- **Complex Reward Structure**: Rewards are based on various factors like level completion, remaining time, etc.
- **In-Game Challenges**: The presence of traps such as holes, enemies, and pipes creates a challenging environment for algorithms to navigate.

Using raw images and outputs can be inefficient and overly complex.
In many frames, numerous pixels carry no useful information. 
To make the learning process more efficient, the gym plateform provides simplifications to interact with the game:

- **Emulator**: the environment provide us an emulator (Nes-py) to run our simulation.
It also wraps the emulator API in a simple way to retrieve frames and receive controller inputs.
- **Simplified Output State**: We can choose each available output actions for the model.
- **Rewards**: The environment will provide rewards for moving to the right, aligning with the game's objective.
- **Exclude Non-Essential Elements**: Remove loading screens and cut-scenes to focus solely on gameplay.
- **Downsampled Game Version**: Use a lower resolution version of the game to reduce input complexity. We'll use the `SuperMarioBros-v3` simplified version.


We will create a neural network that processes raw game images as input and gamepad events as output:
- For the input, we'll need as many neurons as there are pixel information in a game frame
- For the output, we'll need neurons corresponding to the number of possible actions. 
Each neuron represents a different action, like 'push the move right button', 'push the jump button', etc.
 
Unfortunately, using raw images and raw output seems to be inefficient and too complex. 
On each frames, many pixels carry no usefull information.
We'll simplify environment: 
- Simplified output state: 7 instead of 36
- rewards: the env gives reward if you go right
- No loading screen or cut-scene
- Down sampled version of the game

Key Point: Why Mario Bros is a challenging game for RL algorithms.
- Extracting information from raw images given by the emulator
- Input space is very large (240 x 256 x 3)
- Output space is 5 with `RIGHT_ONLY`
- Internal physics
- Reward is complicated: end of the level, timer, etc 
- Traps: holes, enemies and pipes

```python
# initial state
done, state = False, env.reset()
# main loop
while not done: 
    # the agent select an action given the state
    action = model.predict_action(state)
    # the environment execute the agent action and returns the new state, the reward and if the game is over
    new_state, reward, done = env.step(action)

    # store everything in the replay buffer
    model.replay_buffer.store(state, action, reward, new_state, done)
    model.learn()

    # setting the new_state as the actual state
    state = new_state
```
## How the reward are computed?

## Main problems to solve

- The environment is complicated: 
  - often, the player is stcuk by the pipes
  - there are death trap (hole in the ground)
  - an enemies

- The example shown at the top of the page is a good episode, but it doesn't mean the model has converged to a state where it plays only good episodes. 

## Next steps

In the next article we will talk about DDQN and how it works.
In the following articles we will talk about other algorithms I'd like to try: PPO and probably Neat(? neuroevolution)
PPO is interesting because it's the algorithm behind RLHF used to train the GPT models 

See you next time