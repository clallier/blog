---
layout: post
title:  "Learning to play Mario Bros with reinforcement!"
date:   2023-12-25 22:11:33 +0100
categories: Reinforcement Learning, Machine Learning, PPO, DDQN
image: /assets/img/mario_rl_mini.png
---

## Introduction

This is a project I wanted to make since a long time.
Reinforcement learning is fascinating.
Let a neural network to converge using only the backpropagation and a reward from the environment for exploring and solving a task is very interesting!

We'll use a fairly complex environment: the first world of Mario Bros. (Maybe we should start with something simpler but this is very appealing).
I think it's very interessting to build artificial characters in game just letting them interracting with the world.

Here is an example of what we will make:

<!-- https://giphy.com/gifs/x77AsI0PPARBrrO6vK -->
{% include giphy.html id='x77AsI0PPARBrrO6vK' %}
*Best attempt so far using a DDQN (after 29155 games)*

## Mario Bros as an RL Challenge

The environment is available in [Gym Super Mario Bros](https://pypi.org/project/gym-super-mario-bros/), based on [OpenAI Gym API](https://www.gymlibrary.dev/index.html)

Gym is now replaced with [Gymnasium](https://gymnasium.farama.org/)

We use a simplifed environment: 
- Simplified output state: 7 instead of 36 $6^2$ ?  
- rewards: the env gives reward if you go right
- No loading screen or cut-scene
- Down sampled version of the game

Key Point: Why Mario Bros is a challenging game for RL algorithms.
- Extracting information from raw images given by the emulator
- Input space is very large (240 x 256 x 3)
- Output space is 5 with `RIGHT_ONLY`
- Internal physics
- Reward is complicated: end of the level, timer, etc 
- Traps: holes, enemies and pipes

```python
# initial state
done, state = False, env.reset()
# main loop
while not done: 
    # the agent select an action given the state
    action = model.predict_action(state)
    # the environment execute the agent action and returns the new state, the reward and if the game is over
    new_state, reward, done = env.step(action)

    # store everything in the replay buffer
    model.replay_buffer.store(state, action, reward, new_state, done)
    model.learn()

    # setting the new_state as the actual state
    state = new_state
```
## How the reward are computed?

## Main problems to solve

- The environment is complicated: 
  - often, the player is stcuk by the pipes
  - there are death trap (hole in the ground)
  - an enemies

- The example shown at the top of the page is a good episode, but it doesn't mean the model has converged to a state where it plays only good episodes. 

## Next steps

In the next article we will talk about DDQN and how it works.
In the following articles we will talk about other algorithms I'd like to try: PPO and probably Neat(? neuroevolution)
PPO is interesting because it's the algorithm behind RLHF used to train the GPT models 

See you next time