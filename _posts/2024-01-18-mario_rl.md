---
layout: post
title:  "Learning to play Mario Bros with reinforcement!"
date:   2024-01-18 21:00:00 +0100
categories: Reinforcement Learning, Machine Learning, PPO, DDQN
image: /assets/img/mario_rl_mini.png
---

Let's have a computer to play `Mario Bros` using reinforcement learning.
In this experiment, we'll explore neural networks and test if a computer can master the game.
Ready to level up?

<!-- https://giphy.com/gifs/x77AsI0PPARBrrO6vK -->
{% include giphy.html id='x77AsI0PPARBrrO6vK' %}
*Best attempt so far using a DDQN (after 29155 games)*


## Introduction

Learning to play a game, such as Mario Bros, using reinforcement leanring is a project I wanted to start for a long time.
Neural networks have always fascinated me.
My PhD research focused on applying neural network to graphs, but the field of neural network is remarkably vast.
For example, I've never explored the world of reinforcement learning (RL).

In machine learning, there are three main ways to teach an algorithm:
1. **Supervised learning**: this is the most intuitive method. 
It's similar to teaching a child to recognise letters. 
For each letter they might encounter, you would say `this is an 'A'`, `this is a 'B'`, and so on.

2. **Unsupervised learning**: here, algorithms discover patterns on their own. 
Imagine children playing with shape-sorter toys. 
They figure out how to match shapes with the correct holes by focusing on key features like the shape, while ignoring irrelevant ones like color or material.

3. **Reinforcement learning**: in this approach, an algorithm learns to perform tasks on its own by receiving rewards from the environment.
Video games are an ideal fit for this method: each in-game action will result in a modification of the score, providing a reward, which guide the learning algorithm to explore and exploit the game's environment.

We talked about machine learning and neural networks but what is the difference ?
 
On one hand, `machine learning` is a broad concept: it includes any algorithms that learn to resolve a given task.
In this context, learning is the action of adjusting the internal values of the algorithm.
**Neural network** are a specific type of learning agorithms.
They are a simplified version of biological neural networks, and are usually structured in layers.
 - The first layer is the `input`: it's where the data enters. 
 For example, in a network designed to recognize letters from images.
 their would be one neuron for each pixel value. 
 - The last layer is the `ouput` of the network. 
 In our letter recognition example, it will have 26 output neurons. 
 Each representing a different letter.
 - All the intermediate layers do the network's calculations. 
 - Each neuron it connected with a certain `strength` (called `weight`) to some neurons in the next layer.
 The strengh is usually encoded between 0 (no connection) to 1 (full connection).
 - When data enters the input layer, it travels through each layers and activate some neurons in the output layers.
 - A neuron receives an `activation level` (a value) and propagate its activation, given the strength of the connections, to the next neurons.
 - Learning occus through a process called `error backpropagation`, which alter the strenght a neuron propagates its activation.
 This process turns on or off each neuron's activation on the following neurons.

My goal with this project is to deepen my understanding of reinforcement learning and its main algorithms, and to share about what I learnt.

It's a neat challenge to see if such an algorithm can eventually solve the game.

## Mario Bros as a Reinforcement Learning Challenge

First, let's examine the tools we'll use.
Our main tool is OpenAI'the Gym platform.
Which runs simulations and provides feedback from them.
We'll learn how to get feedback from a Gym environment. 
This includes interacting with the emulator to receive rewards and understanding how these rewards are calculated. 

In the future, we might choose not be able to use the Gym platform (for example, for a different challenge). 
Then, we'll have to handle these aspects ourselves.

### The Gym plateform and Nes-py emulator

Standard plateforms to learn reinforcement learning include [OpenAI Gym API](https://www.gymlibrary.dev/index.html) and its successor [Gymnasium](https://gymnasium.farama.org/).
They offer a [Super Mario Bros. environment](https://pypi.org/project/gym-super-mario-bros/) based on the [Nes-py emulator](https://github.com/Kautenja/nes-py)

This Mario Bros environment is an ideal testing ground due to its balance between simplicity and complexity.
Its traps and enemies provide clear, yet challenging objectives for algorithms.
The main complexities include:

- **Game Physics**: The game's internal physics adds complexity to movement and interaction.
- **Complex Reward**: Rewards depends on factors like level completion, remaining time, etc.
- **In-Game Challenges**: The presence of traps, such as holes, enemies, and pipes, creates a challenging environment for algorithms.

Plus, using raw images and outputs can be inefficient and overly complex.
In many frames, numerous pixels don't carry useful information. 
To make the learning process more efficient, the Gym plateform provides simplifications to interact with the game:

- **Emulator**: the environment provide us an emulator (Nes-py) to run our simulation.
It also wraps the emulator API in a simple way to retrieve frames and send controller inputs.
- **Simplified output**: We can choose each available output actions for the model. 
- **Rewards system**: The environment will provide rewards aligned with the game's objective. We'll explain that in the next section.
- **Environment abstration**: Remove loading screens and cut-scenes to focus solely on gameplay.
- **Data reduction**: Use a lower resolution version of the game to reduce input complexity. We'll use the `SuperMarioBros-v3` simplified version.

![Simplified environment](https://pypi-camo.freetls.fastly.net/51975e7cc634efb02ed92acfb56368733b25f4d9/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f323138343436392f34303934383831372d33636436363030612d363833302d313165382d386162622d3963656536613331643337372e706e67)

We will create a neural network that processes raw game images as input and translate them into gamepad events as output:
- **Input**: Initially the input requires a large number of neurons, one for each piece of picel information in a game frame -
Typically (240 x 256 x 3) or 184.320 input neurons.
By downsampling the frame resolution and convert it to grayscale, we'll simplifying this to (30 x 32 x 1), or 960 neurons.
- **Output**: The number of output neurons will correspond to the possible actions. 
Each neuron will represents a different action, like 'push the move right button', 'push the jump button', etc.
We'll use a simplified output mode called `RIGHT_ONLY`, which has 5 possibilities: 
```python
[['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B']]
```
`Noop` is for no input. 
These simplifications significantly reduces the complexity of the input and output spaces.

Here is an example of how we can interact with the game simulation using the Gym plateform:

```python
# initial state
done, state = False, env.reset()
# main loop
while not done: 
    
    # Here we get a random action 
    # In practice the agent would select an action based on the current state
    # For instance: action = model.predict_action(state)
    action = env.action_space.sample()

    # The environment execute the agent action and returns the new state, the reward and whether the game is over
    new_state, reward, done, info = env.step(action)

    # Will render the new_state
    env.render()

    # Update the current state to the new state
    state = new_state
```

In the described setup, game frames are used as states in the environment:
- `state`: is the previous frame.
- `new_frame` is the next one after the agent action.

Additionally, `Info`  a dictionary that holds various current game information, useful for creating custom reward functions or for tracking the progress of the agent:
- `flag_get`: indicates if the player has completed the level.
- The count of collected coins.
- The number of remaining lifes.
- The in-game score so far.
- The Mario's status: `small`, `tall`, `fireball`, etc., indicating his current power-up state.
- the remaining time on the clock.
- `x_pos` and `y_pos`: these represent Mario's position on the stage, providing spatial context within the game

Understanding this information can be crucial for developing custom reward strategy.
For now, it's important to first comprehend how the current reward is computed within the environment.

### How the reward is computed?

The reward is a critical point in the learning process. Let's review how it is computed.
From the [Gym Super Mario Bros page](https://pypi.org/project/gym-super-mario-bros/):

    The reward function assumes the objective of the game is to move as far right as possible (increase the agent's x value), as fast as possible, without dying.

The reward system in this Mario Bros game environment is structured as a sum of three components:
One positive: the velocity, and two negatives: the clock and the death:

1. The `velocity`, noted $v$, is the instantaneous velocity for the given step. It is computed by taking the difference in agent x values between states: $v = x1 - x0$, where:
  - $x0$ is the x position before the step
  - $x1$ is the x position after the step
  - This implies that, when moving right: $v > 0$, when moving left: $v < 0$ and not moving: $v = 0$.

2. The `clock`, noted $c$, is a penalty that prevents the agent from standing still.
We want that when no clock tick: $c = 0$ and then there is a clock tick: $c < 0$.
This is the difference in the game clock between frames: $c = c0 - c1$, where:
  - $c0$ is the clock reading before the step
  - $c1$ is the clock reading after the step

3. The `death` part, denoted $d$, penalizes for dying. This is to encouraging the agent to avoid death. We simply want that whenn the agent is alive: $d = 0$ and when it lost a live: $d = -15$. 

The total reward, $r$, is simply the sum at each frame of the three terms: $r = v + c + d$.
The reward is also clipped into the range (-15, 15). 
This clipping is typically employed to maintain stability in the learning process.
By constraining the reward to a fixed range, it prevents the learning algorithm from being influenced by extreme reward values.

## Next steps

The video shown at the top of the page is a good episode, but it doesn't mean the model has converged to a state where it plays only good episodes. 

In the following articles we'll talk about other algorithms I'd like to play with: Double Deep Q Networks (DDQN), Proximal Policy Optimization (PPO, which is interesting because it's the algorithm behind RLHF used to train the GPT models) and probably NeuroEvolution of Augmenting Topologies (NEAT, which is not an reinforment learning algorithm, it doesn't use the error retropropagation, but seems cool and very efficient).

In the next article we will talk about DDQN and how it works.

Thanks and see you next time!